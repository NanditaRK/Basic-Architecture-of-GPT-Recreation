{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "iQJVufziGXJO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# very simple character-level tokenizer aka each character is condsdired a token\n",
        "class Tokenizer:\n",
        "    def __init__(self,text):\n",
        "        chars = sorted(list(set(text)))\n",
        "        self.str_to_ind = {char: index for index, char in enumerate(chars)}\n",
        "\n",
        "\n",
        "        self.ind_to_str = {index: char for char, index in self.str_to_ind.items()}\n",
        "        self.vocab_size = len(chars)\n",
        "\n",
        "\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.str_to_ind[char] for char in text]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return ''.join([self.ind_to_str[token] for token in tokens])\n"
      ],
      "metadata": {
        "id": "EJDIKvBkGaPt"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embed(nn.Module):\n",
        "    def __init__(self,vocab_size,embed_dim, max_len):\n",
        "\n",
        "\n",
        "        super().__init__()\n",
        "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = nn.Embedding(max_len, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        B, T = x.shape\n",
        "        token_embeddings = self.token_embed(x)\n",
        "\n",
        "        positions = torch.arange(T, device=x.device).unsqueeze(0)\n",
        "        pos_embeddings = self.pos_embed(positions)\n",
        "\n",
        "        return token_embeddings + pos_embeddings  #(B, T, embed_dim)\n"
      ],
      "metadata": {
        "id": "uQXABYZXGw_8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self,embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)  # for query, key, value\n",
        "\n",
        "\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape  # batch, time, dim\n",
        "        qkv = self.qkv(x)  # (B, T, 3D)\n",
        "\n",
        "\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # the different heads\n",
        "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, H, T, d)\n",
        "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # scaled dot-pridcut attention\n",
        "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, H, T, T)\n",
        "\n",
        "\n",
        "        mask = torch.tril(torch.ones(T, T)).to(x.device) == 0  # causal mask\n",
        "        scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        out = attn @ v  # (B, H, T, d)\n",
        "\n",
        "        # concatenate the heads adter\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "        return self.out_proj(out)  # (B, T, D)\n"
      ],
      "metadata": {
        "id": "U9wn-rB5G3CZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim,hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "\n",
        "\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "_MJQtKI5HEWH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads,ff_hidden_dim):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ff = FeedForward(embed_dim, ff_hidden_dim)\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        # includes the skip connectiuns\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "TyBjUzJNHHNz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self, vocab_size,embed_dim,block_size, n_layers,n_heads,ff_hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.embed = Embed(vocab_size, embed_dim, block_size)\n",
        "\n",
        "\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(embed_dim, n_heads, ff_hidden_dim)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "\n",
        "    def generate(self, x, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            x_cond = x[:, -block_size:]  # crop to block size\n",
        "            logits = self(x_cond)\n",
        "\n",
        "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            x = torch.cat([x, next_token], dim=1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "hgzZQBHgHOCN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# corpus from the odyssey book from project gutenburg\n",
        "\n",
        "\n",
        "\n",
        "url = \"https://www.gutenberg.org/cache/epub/1727/pg1727.txt\"\n",
        "\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "response.encoding = 'utf-8'\n",
        "\n",
        "raw_text = response.text\n",
        "\n",
        "# header and footer\n",
        "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK THE ODYSSEY ***\"\n",
        "\n",
        "\n",
        "\n",
        "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK THE ODYSSEY ***\"\n",
        "\n",
        "start = raw_text.find(start_marker)\n",
        "\n",
        "end = raw_text.find(end_marker)\n",
        "\n",
        "if start != -1 and end != -1:\n",
        "    corpus = raw_text[start + len(start_marker):end]\n",
        "\n",
        "\n",
        "else:\n",
        "    corpus = raw_text\n",
        "\n",
        "# clean\n",
        "corpus = corpus.lower()"
      ],
      "metadata": {
        "id": "rbSpvjDAI0Mu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = Tokenizer(corpus)\n",
        "data = tokenizer.encode(corpus)\n",
        "\n",
        "block_size = 8  # context size\n",
        "def get_batch(batch_size=16):\n",
        "    X, Y = [], []\n",
        "    for _ in range(batch_size):\n",
        "\n",
        "\n",
        "        idx = random.randint(0, len(data) - block_size - 1)\n",
        "\n",
        "\n",
        "        x = data[idx:idx+block_size]\n",
        "        y = data[idx+1:idx+block_size+1]\n",
        "\n",
        "        X.append(x)\n",
        "        Y.append(y)\n",
        "    return torch.tensor(X), torch.tensor(Y)\n"
      ],
      "metadata": {
        "id": "yQkH1Uo-H3MH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MiniGPT(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    embed_dim=64,\n",
        "    block_size=block_size,\n",
        "    n_layers=2,\n",
        "    n_heads=4,\n",
        "    ff_hidden_dim=128\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# traingin\n",
        "for step in range(300):\n",
        "    model.train()\n",
        "    xb, yb = get_batch()\n",
        "\n",
        "    logits = model(xb)\n",
        "    B, T, C = logits.shape\n",
        "    loss = loss_fn(logits.view(B*T, C), yb.view(B*T))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        print(f\"Step {step} Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMj7QtvQICdS",
        "outputId": "7a96d2be-c001-4a1a-b653-364d0313a237"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0 Loss: 4.9370\n",
            "Step 50 Loss: 3.0159\n",
            "Step 100 Loss: 2.4031\n",
            "Step 150 Loss: 2.5680\n",
            "Step 200 Loss: 2.1871\n",
            "Step 250 Loss: 2.3408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "context = torch.tensor([[tokenizer.str_to_ind['h']]], dtype=torch.long)  # starting char\n",
        "generated = model.generate(context, max_new_tokens=100)[0].tolist()\n",
        "\n",
        "print(\"\\nGenerated Text:\\n\", tokenizer.decode(generated))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWSjFZiGIGwm",
        "outputId": "0de985c0-aa4f-488a-8a22-3f39c42c0af2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            " henar thacis oul.isg wis, athe at thelal \n",
            "ung ave hacoghanc atede thems lar:f,and tharele tiln- le fi\n"
          ]
        }
      ]
    }
  ]
}